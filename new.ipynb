{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\ayman\\anaconda3\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\ayman\\anaconda3\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ayman\\anaconda3\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ayman\\anaconda3\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ayman\\anaconda3\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ayman\\anaconda3\\lib\\site-packages (from requests) (2024.12.14)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\ayman\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8706 URLs in the sitemap.\n",
      "Sample URLs:\n",
      "https://wuzzuf.net/jobs/p/O2kHWakrqYON-School-Head-of-Admissions-Egypt-Education-Platform-Riyadh-Saudi-Arabia\n",
      "https://wuzzuf.net/jobs/p/Klp23yDjQD9Q-Senior-Backend-Developer-RocketDevs-Cairo-Egypt\n",
      "https://wuzzuf.net/jobs/p/gFlUQx2f9TA2-Civil-Engineer-SAAB-Engineering-Consulating-Office-Giza-Egypt\n",
      "https://wuzzuf.net/jobs/p/3DyZO1fvf2Kp-Senior-Tendering-Engineer-REDCON-Construction-Co-S-A-E-Cairo-Egypt\n",
      "https://wuzzuf.net/jobs/p/kJMPKoPxaAy0-Senior-Android-Developer-Fixed-Solutions-Cairo-Egypt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the sitemap\n",
    "sitemap_url = \"https://wuzzuf.net/sitemap-job-1.xml\"\n",
    "\n",
    "# Fetch the sitemap\n",
    "response = requests.get(sitemap_url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the XML content\n",
    "    soup = BeautifulSoup(response.content, \"xml\")\n",
    "    \n",
    "    # Find all <loc> tags (which contain the URLs)\n",
    "    loc_tags = soup.find_all(\"loc\")\n",
    "    \n",
    "    # Extract the URLs from the <loc> tags\n",
    "    urls = [loc.text for loc in loc_tags]\n",
    "    \n",
    "    # Print the number of URLs found\n",
    "    print(f\"Found {len(urls)} URLs in the sitemap.\")\n",
    "    \n",
    "    # Print the first few URLs as a sample\n",
    "    print(\"Sample URLs:\")\n",
    "    for url in urls[:5]:  # Print the first 5 URLs\n",
    "        print(url)\n",
    "else:\n",
    "    print(f\"Failed to fetch the sitemap. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URLs saved to 'wuzzuf_job_urls.txt'.\n"
     ]
    }
   ],
   "source": [
    "# Save the URLs to a file\n",
    "with open(\"wuzzuf_job_urls.txt\", \"w\") as file:\n",
    "    for url in urls:\n",
    "        file.write(url + \"\\n\")\n",
    "\n",
    "print(\"URLs saved to 'wuzzuf_job_urls.txt'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8571 URLs in the sitemap.\n",
      "Sample URLs:\n",
      "https://wuzzuf.net/jobs/p/O2kHWakrqYON-School-Head-of-Admissions-Egypt-Education-Platform-Riyadh-Saudi-Arabia\n",
      "https://wuzzuf.net/jobs/p/Klp23yDjQD9Q-Senior-Backend-Developer-RocketDevs-Cairo-Egypt\n",
      "https://wuzzuf.net/jobs/p/gFlUQx2f9TA2-Civil-Engineer-SAAB-Engineering-Consulating-Office-Giza-Egypt\n",
      "https://wuzzuf.net/jobs/p/3DyZO1fvf2Kp-Senior-Tendering-Engineer-REDCON-Construction-Co-S-A-E-Cairo-Egypt\n",
      "https://wuzzuf.net/jobs/p/kJMPKoPxaAy0-Senior-Android-Developer-Fixed-Solutions-Cairo-Egypt\n",
      "Navigating 1\n",
      "Error navigating to https://wuzzuf.net/jobs/p/O2kHWakrqYON-School-Head-of-Admissions-Egypt-Education-Platform-Riyadh-Saudi-Arabia: HTTPConnectionPool(host='localhost', port=60678): Read timed out. (read timeout=120)\n",
      "Navigating 2\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from time import sleep\n",
    "import csv\n",
    "\n",
    "# Function to safely get text from an element (returns empty string if element not found)\n",
    "def get_element_text(driver, xpath, index=0):\n",
    "    try:\n",
    "        elements = driver.find_elements(By.XPATH, xpath)\n",
    "        if elements and len(elements) > index:\n",
    "            return elements[index].text\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {xpath}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# URL of the sitemap\n",
    "sitemap_url = \"https://wuzzuf.net/sitemap-job-1.xml\"\n",
    "\n",
    "# Fetch the sitemap\n",
    "response = requests.get(sitemap_url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the XML content\n",
    "    soup = BeautifulSoup(response.content, \"xml\")\n",
    "    \n",
    "    # Find all <loc> tags (which contain the URLs)\n",
    "    loc_tags = soup.find_all(\"loc\")\n",
    "    \n",
    "    # Extract the URLs from the <loc> tags\n",
    "    urls = [loc.text for loc in loc_tags]\n",
    "    \n",
    "    # Print the number of URLs found\n",
    "    print(f\"Found {len(urls)} URLs in the sitemap.\")\n",
    "    \n",
    "    # Print the first few URLs as a sample\n",
    "    print(\"Sample URLs:\")\n",
    "    for url in urls[:5]:  # Print the first 5 URLs\n",
    "        print(url)\n",
    "    \n",
    "    # Initialize the WebDriver\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    \n",
    "    # List to store job data\n",
    "    data = []\n",
    "    \n",
    "    # Navigate to each URL and scrape data\n",
    "    i=0\n",
    "    for url in urls[:4]:  # Process the first 2000 URLs for demonstration\n",
    "        i = i+1\n",
    "        print(f\"Navigating {i}\")\n",
    "        try:\n",
    "            # Navigate to the URL\n",
    "            driver.get(url)\n",
    "            \n",
    "            # Wait for the page to load (e.g., wait for the job title to appear)\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.XPATH, \"//section/div/h1\"))\n",
    "            )\n",
    "            \n",
    "            # Scrape job details\n",
    "            job_data = {\n",
    "                \"Title\": get_element_text(driver, \"//section/div/h1\"),\n",
    "                \"Company\": get_element_text(driver, \"//section/div/div/div/a/span\"),\n",
    "                \"Working Place\": get_element_text(driver, \"//section/div/div/a/span\"),\n",
    "                \"Location\": get_element_text(driver, \"//strong/div/a\"),\n",
    "                \"Post Date\": get_element_text(driver, \"//section/div/span\"),\n",
    "                \"Number of Positions\": get_element_text(driver, \"//section/div/div/div/strong\"),\n",
    "                \"Applicants\": get_element_text(driver, \"//div/div/span/span\", index=1),\n",
    "                \"Career Level\": get_element_text(driver, \"//main//section/div/span/span\"),\n",
    "                \"Education\": get_element_text(driver, \"//main//section/div/span/span\", index=1),\n",
    "                \"Salary\": get_element_text(driver, \"//main//section/div/span/span\", index=2),\n",
    "                \"Job Category\": get_element_text(driver, \"//main//section/div/span/span\", index=3),\n",
    "                \"Skills\": get_element_text(driver, \"//div/ul/li/a/span\"),\n",
    "                \"Job Description\": \" \".join([i.text for i in driver.find_elements(By.XPATH, \"//div/a/span/span/span\")]),\n",
    "\n",
    "            }\n",
    "            \n",
    "            # Append the job data to the list\n",
    "            data.append(job_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error navigating to {url}: {e}\")\n",
    "        finally:\n",
    "            # Add a delay between requests to avoid overloading the server\n",
    "            sleep(2)\n",
    "    \n",
    "    # Close the WebDriver\n",
    "    driver.quit()\n",
    "    \n",
    "    # Save the scraped data to a CSV file\n",
    "    with open(\"wuzzuf_jobs.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=job_data.keys())\n",
    "        writer.writeheader()  # Write the header row\n",
    "        writer.writerows(data)  # Write the job data rows\n",
    "    \n",
    "    print(\"Scraped data saved to 'wuzzuf_jobs2.csv'.\")\n",
    "else:\n",
    "    print(f\"Failed to fetch the sitemap. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
